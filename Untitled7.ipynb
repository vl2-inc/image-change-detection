{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7tFrADvdkN-",
        "colab_type": "code",
        "outputId": "b47a2ace-2c18-41e6-d62c-6c7d04a747ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsABxF8Aced7",
        "colab_type": "code",
        "outputId": "c82cfb24-8642-4b2a-8244-a19fa4e52f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "a=image.load_img('/content/drive/My Drive/Fake/Train/Output/Scene0000_View00_gtmask.png')\n",
        "print(np.shape(a))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(112, 112, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUFje3OF73bZ",
        "colab_type": "code",
        "outputId": "c879e855-53d5-4f0a-df37-c9e81b0898bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg19 import VGG19\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "#import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import Input , Flatten , Dense , Dropout\n",
        "from keras.layers import concatenate\n",
        "#from keras import backend as K\n",
        "\n",
        "#input_img1 = Input((224,224,3))\n",
        "#input_img2 = Input((224,224,3))\n",
        "\n",
        "model1 = VGG19(weights = \"imagenet\", include_top=False)\n",
        "model2 = VGG19(weights = \"imagenet\", include_top=False)\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "\n",
        "#x = Flatten()(x)\n",
        "\n",
        "x = Dense(15000, activation=\"relu\")(x)\n",
        "\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "x = Dense(15000, activation=\"relu\")(x)\n",
        "\n",
        "prediction = Dense(12544, activation=\"softmax\")(x)\n",
        "\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "\n",
        "print(VL2_model.summary())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_83 (InputLayer)           (None, None, None, 3 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_84_2 (InputLayer)         (None, None, None, 3 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, None, None, 6 1792        input_83[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_2 (Conv2D)         (None, None, None, 6 1792        input_84_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, None, None, 6 36928       block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_2 (Conv2D)         (None, None, None, 6 36928       block1_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool (MaxPooling2D)      (None, None, None, 6 0           block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool_2 (MaxPooling2D)    (None, None, None, 6 0           block1_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1 (Conv2D)           (None, None, None, 1 73856       block1_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1_2 (Conv2D)         (None, None, None, 1 73856       block1_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2 (Conv2D)           (None, None, None, 1 147584      block2_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2_2 (Conv2D)         (None, None, None, 1 147584      block2_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, None, None, 1 0           block2_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool_2 (MaxPooling2D)    (None, None, None, 1 0           block2_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1 (Conv2D)           (None, None, None, 2 295168      block2_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1_2 (Conv2D)         (None, None, None, 2 295168      block2_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2 (Conv2D)           (None, None, None, 2 590080      block3_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2_2 (Conv2D)         (None, None, None, 2 590080      block3_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3 (Conv2D)           (None, None, None, 2 590080      block3_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3_2 (Conv2D)         (None, None, None, 2 590080      block3_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv4 (Conv2D)           (None, None, None, 2 590080      block3_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv4_2 (Conv2D)         (None, None, None, 2 590080      block3_conv3_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, None, None, 2 0           block3_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool_2 (MaxPooling2D)    (None, None, None, 2 0           block3_conv4_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1 (Conv2D)           (None, None, None, 5 1180160     block3_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1_2 (Conv2D)         (None, None, None, 5 1180160     block3_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2 (Conv2D)           (None, None, None, 5 2359808     block4_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2_2 (Conv2D)         (None, None, None, 5 2359808     block4_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3 (Conv2D)           (None, None, None, 5 2359808     block4_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3_2 (Conv2D)         (None, None, None, 5 2359808     block4_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv4 (Conv2D)           (None, None, None, 5 2359808     block4_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv4_2 (Conv2D)         (None, None, None, 5 2359808     block4_conv3_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, None, None, 5 0           block4_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool_2 (MaxPooling2D)    (None, None, None, 5 0           block4_conv4_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1 (Conv2D)           (None, None, None, 5 2359808     block4_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1_2 (Conv2D)         (None, None, None, 5 2359808     block4_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2 (Conv2D)           (None, None, None, 5 2359808     block5_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2_2 (Conv2D)         (None, None, None, 5 2359808     block5_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3 (Conv2D)           (None, None, None, 5 2359808     block5_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3_2 (Conv2D)         (None, None, None, 5 2359808     block5_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv4 (Conv2D)           (None, None, None, 5 2359808     block5_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv4_2 (Conv2D)         (None, None, None, 5 2359808     block5_conv3_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool (MaxPooling2D)      (None, None, None, 5 0           block5_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool_2 (MaxPooling2D)    (None, None, None, 5 0           block5_conv4_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, None, None, 1 0           block5_pool[0][0]                \n",
            "                                                                 block5_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, None, None, 1 15375000    concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, None, None, 1 0           dense_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, None, None, 1 225015000   dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, None, None, 1 188172544   dense_21[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 468,611,312\n",
            "Trainable params: 428,562,544\n",
            "Non-trainable params: 40,048,768\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5jN7X8jSTxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg19 import VGG19\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import Flatten , Dense , Dropout , concatenate\n",
        "#from keras import backend as K\n",
        "from tqdm import tqdm\n",
        "\n",
        "input_train01 = '/content/drive/My Drive/Fake/Train/Input01'\n",
        "input_train02 = '/content/drive/My Drive/Fake/Train/Input02'\n",
        "output_train = '/content/drive/My Drive/Fake/Train/Output'\n",
        "\n",
        "input_test01 = '/content/drive/My Drive/Fake/Test/Input01'\n",
        "input_test02 = '/content/drive/My Drive/Fake/Test/Input02'\n",
        "output_test = '/content/drive/My Drive/Fake/Test/Output'\n",
        "\n",
        "##Preparamos nuestras imagenes\n",
        "'''\n",
        "datagen = image.ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "input_train01 = datagen.flow_from_directory(input_train01,batch_size=32,class_mode='binary')\n",
        "input_train02 = datagen.flow_from_directory(input_train02,batch_size=32,class_mode='binary')\n",
        "output_train = datagen.flow_from_directory(output_train,batch_size=32,class_mode='binary')\n",
        "\n",
        "input_test01 = datagen.flow_from_directory(input_test01,batch_size=32,class_mode='binary')\n",
        "input_test02 = datagen.flow_from_directory(input_test02,batch_size=32,class_mode='binary')\n",
        "output_test = datagen.flow_from_directory(output_test,batch_size=32,class_mode='binary')\n",
        "'''\n",
        "\n",
        "def data_gen(path,i):\n",
        "  datagen = []\n",
        "  if i==0:\n",
        "    for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats\n",
        "      img_array = cv2.imread(os.path.join(path,img))  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  if i==1:\n",
        "    for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats\n",
        "      img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  return datagen\n",
        "\n",
        "input_train01 = np.array(data_gen(input_train01,0))\n",
        "input_train02 = np.array(data_gen(input_train02,0))\n",
        "output_train = np.array(data_gen(output_train,1))\n",
        "\n",
        "input_test01 = np.array(data_gen(input_test01,0))\n",
        "input_test02 = np.array(data_gen(input_test02,0))\n",
        "output_test = np.array(data_gen(output_test,1))\n",
        "\n",
        "#RED NEURONAL\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "model1 = VGG19(weights = \"imagenet\", include_top=False)\n",
        "model2 = VGG19(weights = \"imagenet\", include_top=False)\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "\n",
        "#x = Flatten()(x)\n",
        "x = Dense(15000, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(15000, activation=\"relu\")(x)\n",
        "prediction = Dense(12544, activation=\"softmax\")(x)\n",
        "\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "  \n",
        "VL2_model.compile(loss='binary_crossentropy',optimizer=optimizers.Adam(lr=0.0005),metrics=['accuracy'])\n",
        "VL2_model.fit(x=[input_train01,input_train02],y=output_train,steps_per_epoch=1000,epochs=20,validation_data=[[input_test01,input_test02],output_test],validation_steps=300)\n",
        "\n",
        "target_dir = '/content/drive/My Drive/modelo/'\n",
        "if not os.path.exists(target_dir):\n",
        "    os.mkdir(target_dir)\n",
        "    VL2_model.save('/content/drive/My Drive/modelo/modelo.h5')\n",
        "    VL2_model.save_weights('/content/drive/My Drive/modelo/pesos.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjYO61ZZfzmD",
        "colab_type": "code",
        "outputId": "dc382a43-b8a6-4d58-e767-e066a8c856a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg19 import VGG19\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import Flatten , Dense , Dropout , concatenate\n",
        "#from keras import backend as K\n",
        "from tqdm import tqdm\n",
        "\n",
        "input_train01 = '/content/drive/My Drive/Fake/Train/Input01'\n",
        "input_train02 = '/content/drive/My Drive/Fake/Train/Input02'\n",
        "output_train = '/content/drive/My Drive/Fake/Train/Output'\n",
        "\n",
        "input_test01 = '/content/drive/My Drive/Fake/Test/Input01'\n",
        "input_test02 = '/content/drive/My Drive/Fake/Test/Input02'\n",
        "output_test = '/content/drive/My Drive/Fake/Test/Output'\n",
        "\n",
        "##Preparamos nuestras imagenes\n",
        "\n",
        "def data_gen(path):\n",
        "  datagen = []\n",
        "  for img in tqdm(os.listdir(path)):  # iterate over each image\n",
        "    #print(img)\n",
        "    img_array = cv2.imread(os.path.join(path,img))  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "    datagen.append(img_array)  # add this to our training_data\n",
        "  return datagen\n",
        "#data_gen(output_test)\n",
        "\n",
        "print(np.shape(data_gen(output_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 227.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(2, 112, 112, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv6AiHghnLR5",
        "colab_type": "code",
        "outputId": "2b2ab5de-d3d3-49ee-89db-bab8f327fece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg19 import VGG19\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "#import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import Input , Convolution2D , Conv2DTranspose , Reshape\n",
        "from keras.layers import concatenate\n",
        "#from keras import backend as K\n",
        "\n",
        "#input_img1 = Input((224,224,3))\n",
        "#input_img2 = Input((224,224,3))\n",
        "\n",
        "model1 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "model2 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "x = Convolution2D(10,(1,1), padding =\"same\", activation='relu')(x)\n",
        "\n",
        "x = Conv2DTranspose(10,(1,1), strides=(16,16) , padding =\"same\" , activation='relu')(x)\n",
        "\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='softmax')(x)\n",
        "\n",
        "prediction = Reshape((112,112))(x)\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "\n",
        "print(VL2_model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_15 (InputLayer)           (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_16_2 (InputLayer)         (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_2 (Conv2D)         (None, 224, 224, 64) 1792        input_16_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_2 (Conv2D)         (None, 224, 224, 64) 36928       block1_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool_2 (MaxPooling2D)    (None, 112, 112, 64) 0           block1_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1_2 (Conv2D)         (None, 112, 112, 128 73856       block1_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2_2 (Conv2D)         (None, 112, 112, 128 147584      block2_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool_2 (MaxPooling2D)    (None, 56, 56, 128)  0           block2_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1_2 (Conv2D)         (None, 56, 56, 256)  295168      block2_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2_2 (Conv2D)         (None, 56, 56, 256)  590080      block3_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3_2 (Conv2D)         (None, 56, 56, 256)  590080      block3_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv4 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv4_2 (Conv2D)         (None, 56, 56, 256)  590080      block3_conv3_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool_2 (MaxPooling2D)    (None, 28, 28, 256)  0           block3_conv4_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1_2 (Conv2D)         (None, 28, 28, 512)  1180160     block3_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2_2 (Conv2D)         (None, 28, 28, 512)  2359808     block4_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3_2 (Conv2D)         (None, 28, 28, 512)  2359808     block4_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv4 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv4_2 (Conv2D)         (None, 28, 28, 512)  2359808     block4_conv3_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool_2 (MaxPooling2D)    (None, 14, 14, 512)  0           block4_conv4_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1_2 (Conv2D)         (None, 14, 14, 512)  2359808     block4_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2_2 (Conv2D)         (None, 14, 14, 512)  2359808     block5_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3_2 (Conv2D)         (None, 14, 14, 512)  2359808     block5_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv4 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv4_2 (Conv2D)         (None, 14, 14, 512)  2359808     block5_conv3_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool_2 (MaxPooling2D)    (None, 7, 7, 512)    0           block5_conv4_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 7, 7, 1024)   0           block5_pool[0][0]                \n",
            "                                                                 block5_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 7, 7, 10)     10250       concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTrans (None, 112, 112, 10) 110         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 112, 112, 1)  11          conv2d_transpose_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "reshape_8 (Reshape)             (None, 112, 112)     0           conv2d_13[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 40,059,139\n",
            "Trainable params: 10,371\n",
            "Non-trainable params: 40,048,768\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"re...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1JCnScuZhoy",
        "colab_type": "code",
        "outputId": "ce91671f-d82d-4de0-b413-fc0df64d506b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg19 import VGG19\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import Flatten , Dense , Dropout , concatenate\n",
        "#from keras import backend as K\n",
        "from tqdm import tqdm\n",
        "\n",
        "input_train01 = '/content/drive/My Drive/Fake/Train/Input01'\n",
        "input_train02 = '/content/drive/My Drive/Fake/Train/Input02'\n",
        "output_train = '/content/drive/My Drive/Fake/Train/Output'\n",
        "\n",
        "input_test01 = '/content/drive/My Drive/Fake/Test/Input01'\n",
        "input_test02 = '/content/drive/My Drive/Fake/Test/Input02'\n",
        "output_test = '/content/drive/My Drive/Fake/Test/Output'\n",
        "\n",
        "##Preparamos nuestras imagenes\n",
        "\n",
        "def data_gen(path,i):\n",
        "  datagen = []\n",
        "  if i==0:\n",
        "    for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats\n",
        "      img_array = cv2.imread(os.path.join(path,img))  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  if i==1:\n",
        "    for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats\n",
        "      img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  return datagen\n",
        "\n",
        "input_train01 = np.array(data_gen(input_train01,0))\n",
        "input_train02 = np.array(data_gen(input_train02,0))\n",
        "output_train = np.array(data_gen(output_train,1))\n",
        "\n",
        "input_test01 = np.array(data_gen(input_test01,0))\n",
        "input_test02 = np.array(data_gen(input_test02,0))\n",
        "output_test = np.array(data_gen(output_test,1))\n",
        "\n",
        "#RED NEURONAL\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "model1 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "model2 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='relu')(x)\n",
        "\n",
        "x = Conv2DTranspose(1,(1,1), strides=(16,16) , padding =\"same\" , activation='softmax')(x)\n",
        "\n",
        "prediction = Reshape((112,112))(x)\n",
        "\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "  \n",
        "VL2_model.compile(loss='binary_crossentropy',optimizer=optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
        "VL2_model.fit(x=[input_train01,input_train02],y=output_train,steps_per_epoch=20,epochs=3,validation_data=[[input_test01,input_test02],output_test],validation_steps=5)\n",
        "\n",
        "target_dir = '/content/drive/My Drive/modelo/'\n",
        "if not os.path.exists(target_dir):\n",
        "    os.mkdir(target_dir)\n",
        "    VL2_model.save('/content/drive/My Drive/modelo/modelo.h5')\n",
        "    VL2_model.save_weights('/content/drive/My Drive/modelo/pesos.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:00<00:00, 182.06it/s]\n",
            "100%|██████████| 8/8 [00:00<00:00, 214.39it/s]\n",
            "100%|██████████| 8/8 [00:00<00:00, 422.63it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 170.40it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 183.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 221.34it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"re...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 8 samples, validate on 2 samples\n",
            "Epoch 1/3\n",
            "20/20 [==============================] - 9s 435ms/step - loss: 11.7698 - acc: 0.0000e+00 - val_loss: 12.0534 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "20/20 [==============================] - 7s 372ms/step - loss: 11.7698 - acc: 0.0000e+00 - val_loss: 12.0534 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "20/20 [==============================] - 7s 371ms/step - loss: 11.7698 - acc: 0.0000e+00 - val_loss: 12.0534 - val_acc: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbmkLPGgbYIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/Data_VL2/dataVL2.txt', header = None)\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "input01 = data[0]\n",
        "input02 = data[1]\n",
        "output = data[2]\n",
        "\n",
        "#img_array = cv2.imread('/content/drive/My Drive/Data_VL2'+output[1])\n",
        "#print(np.shape(img_array))\n",
        "\n",
        "def data_gen(path,i,inicio,fin):\n",
        "  datagen = []\n",
        "  if i==0:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir)  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  if i==1:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir,cv2.IMREAD_GRAYSCALE)  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  return datagen\n",
        "\n",
        "\n",
        "input_train01 = np.array(data_gen(input01,0,0,400))\n",
        "input_train02 = np.array(data_gen(input02,0,0,400))\n",
        "output_train = np.array(data_gen(output,1,0,400))\n",
        "\n",
        "input_test01 = np.array(data_gen(input01,0,400,500))\n",
        "input_test02 = np.array(data_gen(input02,0,400,500))\n",
        "output_test = np.array(data_gen(output,1,400,500))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cigJgCrkN1dH",
        "colab_type": "code",
        "outputId": "bc2ca448-2be8-4c34-d15f-7e9a54727214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(input_test01)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3opOaVimoZB",
        "colab_type": "code",
        "outputId": "d022eb63-0c21-428f-9bb1-f96830c538c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg19 import VGG19\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import concatenate ,Convolution2D , Conv2DTranspose , Reshape\n",
        "#from keras import backend as K\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/Data_VL2/dataVL2.txt', header = None)\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "input01 = data[0]\n",
        "input02 = data[1]\n",
        "output = data[2]\n",
        "\n",
        "#img_array = cv2.imread('/content/drive/My Drive/Data_VL2'+output[1])\n",
        "#print(np.shape(img_array))\n",
        "\n",
        "def data_gen(path,i,inicio,fin,divisor):\n",
        "  datagen = []\n",
        "  if i==0:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir)/divisor  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  if i==1:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir,cv2.IMREAD_GRAYSCALE)/divisor  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  return datagen\n",
        "\n",
        "input_train01 = np.array(data_gen(input01,0,0,400,1))\n",
        "input_train02 = np.array(data_gen(input02,0,0,400,1))\n",
        "output_train = np.array(data_gen(output,1,0,400,255))\n",
        "\n",
        "input_test01 = np.array(data_gen(input01,0,400,500,1))\n",
        "input_test02 = np.array(data_gen(input02,0,400,500,1))\n",
        "output_test = np.array(data_gen(output,1,400,500,255))\n",
        "\n",
        "#RED NEURONAL\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "model1 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "model2 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='relu')(x)\n",
        "\n",
        "x = Conv2DTranspose(1,(1,1), strides=(16,16) , padding =\"same\" , activation='softmax')(x)\n",
        "\n",
        "prediction = Reshape((112,112))(x)\n",
        "\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "  \n",
        "VL2_model.compile(loss='binary_crossentropy',optimizer=optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
        "VL2_model.fit(x=[input_train01,input_train02],y=output_train,steps_per_epoch=20,epochs=3,validation_data=[[input_test01,input_test02],output_test],validation_steps=5)\n",
        "\n",
        "target_dir = '/content/drive/My Drive/modelo/'\n",
        "if not os.path.exists(target_dir):\n",
        "    os.mkdir(target_dir)\n",
        "VL2_model.save('/content/drive/My Drive/modelo/modelo01.h5')\n",
        "VL2_model.save_weights('/content/drive/My Drive/modelo/pesos01.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:80: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"re...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "20/20 [==============================] - 663s 33s/step - loss: 15.9289 - acc: 8.4821e-04 - val_loss: 15.9294 - val_acc: 8.1553e-04\n",
            "Epoch 2/3\n",
            "20/20 [==============================] - 595s 30s/step - loss: 15.9289 - acc: 8.4821e-04 - val_loss: 15.9294 - val_acc: 8.1553e-04\n",
            "Epoch 3/3\n",
            "20/20 [==============================] - 593s 30s/step - loss: 15.9289 - acc: 8.4821e-04 - val_loss: 15.9294 - val_acc: 8.1553e-04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHTlmoxd4gF9",
        "colab_type": "code",
        "outputId": "2103b45f-f438-4347-c73c-00a9a4c5f9c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model\n",
        "\n",
        "modelo = '/content/drive/My Drive/modelo/modelo.h5'\n",
        "pesos_modelo = '/content/drive/My Drive/modelo/pesos.h5'\n",
        "cnn = load_model(modelo)\n",
        "cnn.load_weights(pesos_modelo)\n",
        "\n",
        "x1 = np.array(cv2.imread('/content/drive/My Drive/Data_VL2/Scene0000_View00_moving.png'))\n",
        "x2 = np.array(cv2.imread('/content/drive/My Drive/Data_VL2/Scene0000_View00_target.png'))\n",
        "x1 = [x1]\n",
        "x2 = [x2]\n",
        "array = cnn.predict([x1,x2])\n",
        "\n",
        "#print(np.shape(array[-1]))\n",
        "print(np.min(array[-1]))\n",
        "\n",
        "plt.imshow(array[-1])\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "x = np.array(cv2.imread('/content/drive/My Drive/Data_VL2/Scene0000_View00_gtmask.png'))\n",
        "x=x[:,:,0]\n",
        "y=np.min(x)\n",
        "print(y)\n",
        "#plt.imshow(x[:,:,0])\n",
        "#plt.show()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQGqcEZAa9z99xvXD0/HKZQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nx = np.array(cv2.imread('/content/drive/My Drive/Data_VL2/Scene0000_View00_gtmask.png'))\\nx=x[:,:,0]\\ny=np.min(x)\\nprint(y)\\n#plt.imshow(x[:,:,0])\\n#plt.show()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGlciNkFpRmR",
        "colab_type": "code",
        "outputId": "5e8db475-fcdc-4ee6-e922-1fd1ad4cb070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg19 import VGG19\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import concatenate ,Convolution2D , Conv2DTranspose , Reshape\n",
        "from keras import backend as K\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "def binary_focal_loss(gamma=2., alpha=.25):\n",
        "    \"\"\"\n",
        "    Binary form of focal loss.\n",
        "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
        "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
        "    References:\n",
        "        https://arxiv.org/pdf/1708.02002.pdf\n",
        "    Usage:\n",
        "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
        "    \"\"\"\n",
        "    def binary_focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        :param y_true: A tensor of the same shape as `y_pred`\n",
        "        :param y_pred:  A tensor resulting from a sigmoid\n",
        "        :return: Output tensor.\n",
        "        \"\"\"\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        epsilon = K.epsilon()\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
        "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
        "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
        "\n",
        "    return binary_focal_loss_fixed\n",
        "#-------------------------------------\n",
        "focal_loss = binary_focal_loss(gamma=2., alpha=.25)\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/Data_VL2/dataVL2.txt', header = None)\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "input01 = data[0]\n",
        "input02 = data[1]\n",
        "output = data[2]\n",
        "\n",
        "#img_array = cv2.imread('/content/drive/My Drive/Data_VL2'+output[1])\n",
        "#print(np.shape(img_array))\n",
        "\n",
        "def data_gen(path,i,inicio,fin,divisor):\n",
        "  datagen = []\n",
        "  if i==0:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir)/divisor  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  if i==1:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir,cv2.IMREAD_GRAYSCALE)/divisor  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  return datagen\n",
        "\n",
        "input_train01 = np.array(data_gen(input01,0,0,400,1))\n",
        "input_train02 = np.array(data_gen(input02,0,0,400,1))\n",
        "output_train = np.array(data_gen(output,1,0,400,255))\n",
        "\n",
        "input_test01 = np.array(data_gen(input01,0,400,500,1))\n",
        "input_test02 = np.array(data_gen(input02,0,400,500,1))\n",
        "output_test = np.array(data_gen(output,1,400,500,255))\n",
        "\n",
        "#RED NEURONAL\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "model1 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "model2 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='relu')(x)\n",
        "\n",
        "x = Conv2DTranspose(1,(1,1), strides=(16,16) , padding =\"same\" , activation='relu')(x)\n",
        "\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='softmax')(x)\n",
        "\n",
        "prediction = Reshape((112,112))(x)\n",
        "\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "  \n",
        "#VL2_model.compile(loss='binary_crossentropy',optimizer=optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
        "VL2_model.compile(loss=focal_loss,optimizer=optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
        "VL2_model.fit(x=[input_train01,input_train02],y=output_train,steps_per_epoch=20,epochs=3,validation_data=[[input_test01,input_test02],output_test],validation_steps=5)\n",
        "\n",
        "target_dir = '/content/drive/My Drive/modelo/'\n",
        "if not os.path.exists(target_dir):\n",
        "    os.mkdir(target_dir)\n",
        "VL2_model.save('/content/drive/My Drive/modelo/modelo02.h5')\n",
        "VL2_model.save_weights('/content/drive/My Drive/modelo/pesos02.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"re...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "20/20 [==============================] - 621s 31s/step - loss: 15.9291 - acc: 8.3626e-04 - val_loss: 15.9286 - val_acc: 8.6336e-04\n",
            "Epoch 2/3\n",
            "20/20 [==============================] - 630s 32s/step - loss: 15.9291 - acc: 8.3626e-04 - val_loss: 15.9286 - val_acc: 8.6336e-04\n",
            "Epoch 3/3\n",
            "20/20 [==============================] - 631s 32s/step - loss: 15.9291 - acc: 8.3626e-04 - val_loss: 15.9286 - val_acc: 8.6336e-04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjwJ-EHIYfaR",
        "colab_type": "code",
        "outputId": "737c99dd-615c-4e15-ba62-29c8349621cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg19 import VGG19\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import concatenate ,Convolution2D , Conv2DTranspose , Reshape\n",
        "from keras import backend as K\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "def binary_focal_loss(gamma=2., alpha=.25):\n",
        "    \"\"\"\n",
        "    Binary form of focal loss.\n",
        "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
        "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
        "    References:\n",
        "        https://arxiv.org/pdf/1708.02002.pdf\n",
        "    Usage:\n",
        "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
        "    \"\"\"\n",
        "    def binary_focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        :param y_true: A tensor of the same shape as `y_pred`\n",
        "        :param y_pred:  A tensor resulting from a sigmoid\n",
        "        :return: Output tensor.\n",
        "        \"\"\"\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        epsilon = K.epsilon()\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
        "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
        "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
        "\n",
        "    return binary_focal_loss_fixed\n",
        "#-------------------------------------\n",
        "focal_loss = binary_focal_loss(gamma=2., alpha=.25)\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/Data_VL2/dataVL2.txt', header = None)\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "input01 = data[0]\n",
        "input02 = data[1]\n",
        "output = data[2]\n",
        "\n",
        "#img_array = cv2.imread('/content/drive/My Drive/Data_VL2'+output[1])\n",
        "#print(np.shape(img_array))\n",
        "\n",
        "def data_gen(path,i,inicio,fin,divisor):\n",
        "  datagen = []\n",
        "  if i==0:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir)/divisor  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  if i==1:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir,cv2.IMREAD_GRAYSCALE)/divisor  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  return datagen\n",
        "\n",
        "input_train01 = np.array(data_gen(input01,0,0,400,1))\n",
        "input_train02 = np.array(data_gen(input02,0,0,400,1))\n",
        "output_train = np.array(data_gen(output,1,0,400,255))\n",
        "\n",
        "input_test01 = np.array(data_gen(input01,0,400,500,1))\n",
        "input_test02 = np.array(data_gen(input02,0,400,500,1))\n",
        "output_test = np.array(data_gen(output,1,400,500,255))\n",
        "\n",
        "#RED NEURONAL\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "model1 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "model2 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='relu')(x)\n",
        "\n",
        "x = Conv2DTranspose(1,(1,1), strides=(16,16) , padding =\"same\" , activation='relu')(x)\n",
        "\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='softmax')(x)\n",
        "\n",
        "prediction = Reshape((112,112))(x)\n",
        "\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "  \n",
        "#VL2_model.compile(loss='binary_crossentropy',optimizer=optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
        "VL2_model.compile(loss=focal_loss,optimizer=optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
        "VL2_model.fit(x=[input_train01,input_train02],y=output_train,steps_per_epoch=20,epochs=3,validation_data=[[input_test01,input_test02],output_test],validation_steps=5)\n",
        "\n",
        "target_dir = '/content/drive/My Drive/modelo/'\n",
        "if not os.path.exists(target_dir):\n",
        "    os.mkdir(target_dir)\n",
        "VL2_model.save('/content/drive/My Drive/modelo/modelo03.h5')\n",
        "VL2_model.save_weights('/content/drive/My Drive/modelo/pesos03.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:113: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"re...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "20/20 [==============================] - 626s 31s/step - loss: 59944376.0000 - acc: 0.9626 - val_loss: 14985362.0000 - val_acc: 0.9613\n",
            "Epoch 2/3\n",
            "20/20 [==============================] - 627s 31s/step - loss: 59944376.0000 - acc: 0.9626 - val_loss: 14985362.0000 - val_acc: 0.9613\n",
            "Epoch 3/3\n",
            "20/20 [==============================] - 635s 32s/step - loss: 59944376.0000 - acc: 0.9626 - val_loss: 14985362.0000 - val_acc: 0.9613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iro9lmKJwnDo",
        "colab_type": "code",
        "outputId": "b2afa3b3-3f73-434d-d391-f2d316dc0a45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg19 import VGG19\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import concatenate ,Convolution2D , Conv2DTranspose , Reshape\n",
        "from keras import backend as K\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "def binary_focal_loss(gamma=2., alpha=.25):\n",
        "    \"\"\"\n",
        "    Binary form of focal loss.\n",
        "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
        "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
        "    References:\n",
        "        https://arxiv.org/pdf/1708.02002.pdf\n",
        "    Usage:\n",
        "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
        "    \"\"\"\n",
        "    def binary_focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        :param y_true: A tensor of the same shape as `y_pred`\n",
        "        :param y_pred:  A tensor resulting from a sigmoid\n",
        "        :return: Output tensor.\n",
        "        \"\"\"\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        epsilon = K.epsilon()\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
        "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
        "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
        "\n",
        "    return binary_focal_loss_fixed\n",
        "#-------------------------------------\n",
        "focal_loss = binary_focal_loss(gamma=2., alpha=.25)\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/Data_VL2/dataVL2.txt', header = None)\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "input01 = data[0]\n",
        "input02 = data[1]\n",
        "output = data[2]\n",
        "\n",
        "#img_array = cv2.imread('/content/drive/My Drive/Data_VL2'+output[1])\n",
        "#print(np.shape(img_array))\n",
        "\n",
        "def data_gen(path,i,inicio,fin,divisor):\n",
        "  datagen = []\n",
        "  if i==0:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir)/divisor  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  if i==1:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir,cv2.IMREAD_GRAYSCALE)/divisor  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  return datagen\n",
        "\n",
        "input_train01 = np.array(data_gen(input01,0,0,400,1))\n",
        "input_train02 = np.array(data_gen(input02,0,0,400,1))\n",
        "output_train = np.array(data_gen(output,1,0,400,255))\n",
        "\n",
        "input_test01 = np.array(data_gen(input01,0,400,500,1))\n",
        "input_test02 = np.array(data_gen(input02,0,400,500,1))\n",
        "output_test = np.array(data_gen(output,1,400,500,255))\n",
        "\n",
        "#RED NEURONAL\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "model1 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "model2 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "\n",
        "x = Convolution2D(10,(1,1), padding =\"same\", activation='relu')(x)\n",
        "\n",
        "x = Conv2DTranspose(10,(1,1), strides=(16,16) , padding =\"same\" , activation='relu')(x)\n",
        "\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='softmax')(x)\n",
        "\n",
        "prediction = Reshape((112,112))(x)\n",
        "\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "  \n",
        "#VL2_model.compile(loss='binary_crossentropy',optimizer=optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
        "VL2_model.compile(loss=focal_loss,optimizer=optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
        "VL2_model.fit(x=[input_train01,input_train02],y=output_train,steps_per_epoch=20,epochs=3,validation_data=[[input_test01,input_test02],output_test],validation_steps=5)\n",
        "\n",
        "target_dir = '/content/drive/My Drive/modelo/'\n",
        "if not os.path.exists(target_dir):\n",
        "    os.mkdir(target_dir)\n",
        "VL2_model.save('/content/drive/My Drive/modelo/modelo04.h5')\n",
        "VL2_model.save_weights('/content/drive/My Drive/modelo/pesos04.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:113: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"re...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "20/20 [==============================] - 607s 30s/step - loss: 59943676.0000 - acc: 0.9624 - val_loss: 14986066.0000 - val_acc: 0.9622\n",
            "Epoch 2/3\n",
            "20/20 [==============================] - 607s 30s/step - loss: 59943676.0000 - acc: 0.9624 - val_loss: 14986066.0000 - val_acc: 0.9622\n",
            "Epoch 3/3\n",
            "20/20 [==============================] - 605s 30s/step - loss: 59943676.0000 - acc: 0.9624 - val_loss: 14986066.0000 - val_acc: 0.9622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XolFiRJtNs5S",
        "colab_type": "code",
        "outputId": "36eeee61-a039-421c-d026-6677c571a562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg19 import VGG19\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "#import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import Input , Convolution2D , Conv2DTranspose , Reshape\n",
        "from keras.layers import concatenate\n",
        "\n",
        "model1 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "model2 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "x1 = Conv2DTranspose(10,(1,1), strides=(16,16) , padding =\"same\" , activation='relu')(x1)\n",
        "x2 = Conv2DTranspose(10,(1,1), strides=(16,16) , padding =\"same\" , activation='relu')(x2)\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='softmax')(x)\n",
        "prediction = Reshape((112,112))(x)\n",
        "\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "\n",
        "print(VL2_model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2_2 (InputLayer)          (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_2 (Conv2D)         (None, 224, 224, 64) 1792        input_2_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_2 (Conv2D)         (None, 224, 224, 64) 36928       block1_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool_2 (MaxPooling2D)    (None, 112, 112, 64) 0           block1_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1_2 (Conv2D)         (None, 112, 112, 128 73856       block1_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2_2 (Conv2D)         (None, 112, 112, 128 147584      block2_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool_2 (MaxPooling2D)    (None, 56, 56, 128)  0           block2_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1_2 (Conv2D)         (None, 56, 56, 256)  295168      block2_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2_2 (Conv2D)         (None, 56, 56, 256)  590080      block3_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3_2 (Conv2D)         (None, 56, 56, 256)  590080      block3_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv4 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv4_2 (Conv2D)         (None, 56, 56, 256)  590080      block3_conv3_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool_2 (MaxPooling2D)    (None, 28, 28, 256)  0           block3_conv4_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1_2 (Conv2D)         (None, 28, 28, 512)  1180160     block3_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2_2 (Conv2D)         (None, 28, 28, 512)  2359808     block4_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3_2 (Conv2D)         (None, 28, 28, 512)  2359808     block4_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv4 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv4_2 (Conv2D)         (None, 28, 28, 512)  2359808     block4_conv3_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool_2 (MaxPooling2D)    (None, 14, 14, 512)  0           block4_conv4_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1_2 (Conv2D)         (None, 14, 14, 512)  2359808     block4_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2_2 (Conv2D)         (None, 14, 14, 512)  2359808     block5_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3_2 (Conv2D)         (None, 14, 14, 512)  2359808     block5_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv4 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv4_2 (Conv2D)         (None, 14, 14, 512)  2359808     block5_conv3_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool_2 (MaxPooling2D)    (None, 7, 7, 512)    0           block5_conv4_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 112, 112, 10) 5130        block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 112, 112, 10) 5130        block5_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 112, 112, 20) 0           conv2d_transpose_1[0][0]         \n",
            "                                                                 conv2d_transpose_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 112, 112, 1)  21          concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 112, 112)     0           conv2d_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 40,059,049\n",
            "Trainable params: 10,281\n",
            "Non-trainable params: 40,048,768\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"re...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCcClSCkOblq",
        "colab_type": "code",
        "outputId": "9d42b5bb-e27f-404b-9d72-50fbb13b56b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg19 import VGG19\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import concatenate ,Convolution2D , Conv2DTranspose , Reshape\n",
        "from keras import backend as K\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "'''\n",
        "def binary_focal_loss(gamma=2., alpha=.25):\n",
        "    \"\"\"\n",
        "    Binary form of focal loss.\n",
        "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
        "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
        "    References:\n",
        "        https://arxiv.org/pdf/1708.02002.pdf\n",
        "    Usage:\n",
        "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
        "    \"\"\"\n",
        "    def binary_focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        :param y_true: A tensor of the same shape as `y_pred`\n",
        "        :param y_pred:  A tensor resulting from a sigmoid\n",
        "        :return: Output tensor.\n",
        "        \"\"\"\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        epsilon = K.epsilon()\n",
        "        # clip to prevent NaN's and Inf's\n",
        "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
        "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
        "\n",
        "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
        "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
        "\n",
        "    return binary_focal_loss_fixed\n",
        "#-------------------------------------\n",
        "focal_loss = binary_focal_loss(gamma=2., alpha=.25)\n",
        "'''\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/Data_VL2/dataVL2.txt', header = None)\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "input01 = data[0]\n",
        "input02 = data[1]\n",
        "output = data[2]\n",
        "\n",
        "#img_array = cv2.imread('/content/drive/My Drive/Data_VL2'+output[1])\n",
        "#print(np.shape(img_array))\n",
        "\n",
        "def data_gen(path,i,inicio,fin):\n",
        "  datagen = []\n",
        "  if i==0:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir)/255  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  if i==1:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir,cv2.IMREAD_GRAYSCALE)/255  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  return datagen\n",
        "\n",
        "input_train01 = np.array(data_gen(input01,0,0,400))\n",
        "input_train02 = np.array(data_gen(input02,0,0,400))\n",
        "output_train = np.array(data_gen(output,1,0,400))\n",
        "\n",
        "input_test01 = np.array(data_gen(input01,0,400,500))\n",
        "input_test02 = np.array(data_gen(input02,0,400,500))\n",
        "output_test = np.array(data_gen(output,1,400,500))\n",
        "\n",
        "#RED NEURONAL\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "model1 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "model2 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "x1 = Conv2DTranspose(10,(1,1), strides=(16,16) , padding =\"same\" , activation='relu')(x1)\n",
        "x2 = Conv2DTranspose(10,(1,1), strides=(16,16) , padding =\"same\" , activation='relu')(x2)\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='softmax')(x)\n",
        "prediction = Reshape((112,112))(x)\n",
        "\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "  \n",
        "VL2_model.compile(loss='categorical_crossentropy',optimizer=optimizers.SGD(lr=0.00001),metrics=['accuracy'])\n",
        "#VL2_model.compile(loss=focal_loss,optimizer=optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
        "VL2_model.fit(x=[input_train01,input_train02],y=output_train,steps_per_epoch=20,epochs=3,validation_data=[[input_test01,input_test02],output_test],validation_steps=5)\n",
        "\n",
        "target_dir = '/content/drive/My Drive/modelo/'\n",
        "if not os.path.exists(target_dir):\n",
        "    os.mkdir(target_dir)\n",
        "VL2_model.save('/content/drive/My Drive/modelo/modelo06.h5')\n",
        "VL2_model.save_weights('/content/drive/My Drive/modelo/pesos06.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:112: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"re...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "20/20 [==============================] - 698s 35s/step - loss: 0.4511 - acc: 0.9621 - val_loss: 0.4196 - val_acc: 0.9633\n",
            "Epoch 2/3\n",
            "20/20 [==============================] - 694s 35s/step - loss: 0.4511 - acc: 0.9621 - val_loss: 0.4196 - val_acc: 0.9633\n",
            "Epoch 3/3\n",
            "20/20 [==============================] - 703s 35s/step - loss: 0.4511 - acc: 0.9621 - val_loss: 0.4196 - val_acc: 0.9633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIRdNdBO88mn",
        "colab_type": "code",
        "outputId": "3a2aad8e-da2d-45ae-de65-ee07bcca6352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg19 import VGG19\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "#import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import BatchNormalization , Convolution2D , Conv2DTranspose , UpSampling2D , Reshape\n",
        "from keras.layers import concatenate\n",
        "\n",
        "model1 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "model2 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Convolution2D(1,(1,1), padding =\"same\", activation='relu')(x1)\n",
        "x2 = Convolution2D(1,(1,1), padding =\"same\", activation='relu')(x2)\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(16,(3,3), strides=(1,1) , padding =\"same\" , activation='relu')(x1)\n",
        "x2 = Conv2DTranspose(16,(3,3), strides=(1,1) , padding =\"same\" , activation='relu')(x2)\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(16,(3,3), strides=(1,1) , activation='relu')(x1)\n",
        "x2 = Conv2DTranspose(16,(3,3), strides=(1,1) , activation='relu')(x2)\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(32,(3,3), strides=(1,1) , activation='relu')(x1)\n",
        "x2 = Conv2DTranspose(32,(3,3), strides=(1,1) , activation='relu')(x2)\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(64,(2,2), strides=(1,1) , activation='relu')(x1)\n",
        "x2 = Conv2DTranspose(64,(2,2), strides=(1,1) , activation='relu')(x2)\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(1,(3,3), strides=(1,1) , activation='relu')(x1)\n",
        "x2 = Conv2DTranspose(1,(3,3), strides=(1,1) , activation='relu')(x2)\n",
        "\n",
        "x1 = UpSampling2D(size=(8,8), interpolation='bilinear')(x1)\n",
        "x2 = UpSampling2D(size=(8,8), interpolation='bilinear')(x2)\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='softmax')(x)\n",
        "prediction = Reshape((112,112))(x)\n",
        "\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "\n",
        "print(VL2_model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_31 (InputLayer)           (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_32_2 (InputLayer)         (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_31[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_2 (Conv2D)         (None, 224, 224, 64) 1792        input_32_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_2 (Conv2D)         (None, 224, 224, 64) 36928       block1_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool_2 (MaxPooling2D)    (None, 112, 112, 64) 0           block1_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1_2 (Conv2D)         (None, 112, 112, 128 73856       block1_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2_2 (Conv2D)         (None, 112, 112, 128 147584      block2_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool_2 (MaxPooling2D)    (None, 56, 56, 128)  0           block2_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1_2 (Conv2D)         (None, 56, 56, 256)  295168      block2_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2_2 (Conv2D)         (None, 56, 56, 256)  590080      block3_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3_2 (Conv2D)         (None, 56, 56, 256)  590080      block3_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv4 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv4_2 (Conv2D)         (None, 56, 56, 256)  590080      block3_conv3_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool_2 (MaxPooling2D)    (None, 28, 28, 256)  0           block3_conv4_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1_2 (Conv2D)         (None, 28, 28, 512)  1180160     block3_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2_2 (Conv2D)         (None, 28, 28, 512)  2359808     block4_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3_2 (Conv2D)         (None, 28, 28, 512)  2359808     block4_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv4 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv4_2 (Conv2D)         (None, 28, 28, 512)  2359808     block4_conv3_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool_2 (MaxPooling2D)    (None, 14, 14, 512)  0           block4_conv4_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1_2 (Conv2D)         (None, 14, 14, 512)  2359808     block4_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2_2 (Conv2D)         (None, 14, 14, 512)  2359808     block5_conv1_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3_2 (Conv2D)         (None, 14, 14, 512)  2359808     block5_conv2_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv4 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv4_2 (Conv2D)         (None, 14, 14, 512)  2359808     block5_conv3_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool_2 (MaxPooling2D)    (None, 7, 7, 512)    0           block5_conv4_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 7, 7, 512)    2048        block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 7, 7, 512)    2048        block5_pool_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 7, 7, 1)      513         batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 7, 7, 1)      513         batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 7, 7, 1)      4           conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 7, 7, 1)      4           conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_141 (Conv2DTra (None, 7, 7, 16)     160         batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_142 (Conv2DTra (None, 7, 7, 16)     160         batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 7, 7, 16)     64          conv2d_transpose_141[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 7, 7, 16)     64          conv2d_transpose_142[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_143 (Conv2DTra (None, 9, 9, 16)     2320        batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_144 (Conv2DTra (None, 9, 9, 16)     2320        batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 9, 9, 16)     64          conv2d_transpose_143[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 9, 9, 16)     64          conv2d_transpose_144[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_145 (Conv2DTra (None, 11, 11, 32)   4640        batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_146 (Conv2DTra (None, 11, 11, 32)   4640        batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 11, 11, 32)   128         conv2d_transpose_145[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 11, 11, 32)   128         conv2d_transpose_146[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_147 (Conv2DTra (None, 12, 12, 64)   8256        batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_148 (Conv2DTra (None, 12, 12, 64)   8256        batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 12, 12, 64)   256         conv2d_transpose_147[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 12, 12, 64)   256         conv2d_transpose_148[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_149 (Conv2DTra (None, 14, 14, 1)    577         batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_150 (Conv2DTra (None, 14, 14, 1)    577         batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_26 (UpSampling2D) (None, 112, 112, 1)  0           conv2d_transpose_149[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_27 (UpSampling2D) (None, 112, 112, 1)  0           conv2d_transpose_150[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 112, 112, 2)  0           up_sampling2d_26[0][0]           \n",
            "                                                                 up_sampling2d_27[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 112, 112, 1)  3           concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 112, 112)     0           conv2d_42[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 40,086,831\n",
            "Trainable params: 35,499\n",
            "Non-trainable params: 40,051,332\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:75: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"re...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXJcXTURTs-w",
        "colab_type": "code",
        "outputId": "fb7a8ec5-759a-4d2a-cb83-f400764fc495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg19 import VGG19\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import concatenate ,Convolution2D ,Conv2DTranspose ,Reshape ,Dropout ,BatchNormalization ,UpSampling2D\n",
        "from keras import backend as K\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/Data_VL2/dataVL2.txt', header = None)\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "input01 = data[0]\n",
        "input02 = data[1]\n",
        "output = data[2]\n",
        "\n",
        "def data_gen(path,i,inicio,fin):\n",
        "  datagen = []\n",
        "  if i==0:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir)/255  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  if i==1:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir,cv2.IMREAD_GRAYSCALE)/255  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  return datagen\n",
        "\n",
        "input_train01 = np.array(data_gen(input01,0,0,400))\n",
        "input_train02 = np.array(data_gen(input02,0,0,400))\n",
        "output_train = np.array(data_gen(output,1,0,400))\n",
        "\n",
        "input_test01 = np.array(data_gen(input01,0,400,500))\n",
        "input_test02 = np.array(data_gen(input02,0,400,500))\n",
        "output_test = np.array(data_gen(output,1,400,500))\n",
        "\n",
        "#RED NEURONAL\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "model1 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "model2 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Convolution2D(1,(1,1), padding =\"same\", activation='relu',kernel_initializer='he_uniform')(x1)\n",
        "x2 = Convolution2D(1,(1,1), padding =\"same\", activation='relu',kernel_initializer='he_uniform')(x2)\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(16,(3,3), strides=(1,1) , padding =\"same\" , activation='relu',kernel_initializer='he_uniform')(x1)\n",
        "x2 = Conv2DTranspose(16,(3,3), strides=(1,1) , padding =\"same\" , activation='relu',kernel_initializer='he_uniform')(x2)\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(16,(3,3), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x1)\n",
        "x2 = Conv2DTranspose(16,(3,3), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x2)\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(32,(3,3), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x1)\n",
        "x2 = Conv2DTranspose(32,(3,3), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x2)\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(64,(2,2), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x1)\n",
        "x2 = Conv2DTranspose(64,(2,2), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x2)\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(1,(3,3), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x1)\n",
        "x2 = Conv2DTranspose(1,(3,3), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x2)\n",
        "\n",
        "x1 = UpSampling2D(size=(8,8), interpolation='bilinear')(x1)\n",
        "x2 = UpSampling2D(size=(8,8), interpolation='bilinear')(x2)\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "x = Dropout(0.5)(x)\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='softmax')(x)\n",
        "prediction = Reshape((112,112))(x)\n",
        "\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "  \n",
        "VL2_model.compile(loss='categorical_crossentropy',optimizer=optimizers.SGD(lr=0.001),metrics=['accuracy'])\n",
        "#VL2_model.compile(loss=focal_loss,optimizer=optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
        "VL2_model.fit(x=[input_train01,input_train02],y=output_train,steps_per_epoch=20,epochs=3,validation_data=[[input_test01,input_test02],output_test],validation_steps=5)\n",
        "\n",
        "target_dir = '/content/drive/My Drive/modelo/'\n",
        "if not os.path.exists(target_dir):\n",
        "    os.mkdir(target_dir)\n",
        "VL2_model.save('/content/drive/My Drive/modelo/modelo08.h5')\n",
        "VL2_model.save_weights('/content/drive/My Drive/modelo/pesos08.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:112: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"re...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "20/20 [==============================] - 647s 32s/step - loss: 0.4364 - acc: 0.9628 - val_loss: 0.4786 - val_acc: 0.9604\n",
            "Epoch 2/3\n",
            "20/20 [==============================] - 646s 32s/step - loss: 0.4364 - acc: 0.9628 - val_loss: 0.4786 - val_acc: 0.9604\n",
            "Epoch 3/3\n",
            "20/20 [==============================] - 646s 32s/step - loss: 0.4364 - acc: 0.9628 - val_loss: 0.4786 - val_acc: 0.9604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5PZ8dzfpEj5",
        "colab_type": "code",
        "outputId": "fb929ee9-8646-479d-8e58-7c84de55747b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model\n",
        "\n",
        "modelo = '/content/drive/My Drive/modelo/modelo11.h5'\n",
        "pesos_modelo = '/content/drive/My Drive/modelo/pesos11.h5'\n",
        "cnn = load_model(modelo)\n",
        "cnn.load_weights(pesos_modelo)\n",
        "\n",
        "#x1 = np.array(cv2.imread('/content/drive/My Drive/Data_VL2/Scene0000_View00_moving.png'))\n",
        "#x2 = np.array(cv2.imread('/content/drive/My Drive/Data_VL2/Scene0000_View00_target.png'))\n",
        "x1 = np.array(cv2.resize(cv2.imread('/content/drive/My Drive/Data_VL2/XA.jpg'),(224,224)))\n",
        "x2 = np.array(cv2.resize(cv2.imread('/content/drive/My Drive/Data_VL2/XB.jpg'),(224,224)))\n",
        "x1 = [x1]\n",
        "x2 = [x2]\n",
        "array = cnn.predict([x1,x2])\n",
        "\n",
        "print(np.shape(array[-1]))\n",
        "print(np.max(array[-1]))\n",
        "print(np.unique(array[-1]))\n",
        "plt.imshow(array[-1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(112, 112)\n",
            "1.0\n",
            "[1.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQGqcEZAa9z99xvXD0/HKZQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYXnrTIA0EkH",
        "colab_type": "code",
        "outputId": "25a80a14-8942-4af8-e590-f4a7f1fb7fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "x = np.array(cv2.imread('/content/drive/My Drive/Data_VL2/Scene0000_View00_gtmask.png'))\n",
        "x=x[:,:,0]\n",
        "x=x/255\n",
        "y=np.max(x)\n",
        "print(y)\n",
        "print(np.unique(x))\n",
        "plt.imshow(x)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "[0. 1.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZzU1I_mABr2",
        "colab_type": "code",
        "outputId": "4e71d4b3-2b79-495d-bfe4-c03026bcc91e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg19 import VGG19\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import concatenate ,Convolution2D ,Conv2DTranspose ,Reshape ,Dropout ,BatchNormalization ,UpSampling2D\n",
        "from keras import backend as K\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/Data_VL2/dataVL2.txt', header = None)\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "input01 = data[0]\n",
        "input02 = data[1]\n",
        "output = data[2]\n",
        "\n",
        "def data_gen(path,i,inicio,fin):\n",
        "  datagen = []\n",
        "  if i==0:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir)/255  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  if i==1:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir,cv2.IMREAD_GRAYSCALE)/255  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  return datagen\n",
        "\n",
        "input_train01 = np.array(data_gen(input01,0,0,400))\n",
        "input_train02 = np.array(data_gen(input02,0,0,400))\n",
        "output_train = np.array(data_gen(output,1,0,400))\n",
        "\n",
        "input_test01 = np.array(data_gen(input01,0,400,500))\n",
        "input_test02 = np.array(data_gen(input02,0,400,500))\n",
        "output_test = np.array(data_gen(output,1,400,500))\n",
        "\n",
        "#RED NEURONAL\n",
        "\n",
        "#sess = tf.InteractiveSession()\n",
        "\n",
        "model1 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "model2 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Convolution2D(1,(1,1), padding =\"same\", activation='relu',kernel_initializer='he_uniform')(x1)\n",
        "x2 = Convolution2D(1,(1,1), padding =\"same\", activation='relu',kernel_initializer='he_uniform')(x2)\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(16,(3,3), strides=(1,1) , padding =\"same\" , activation='relu',kernel_initializer='he_uniform')(x1)\n",
        "x2 = Conv2DTranspose(16,(3,3), strides=(1,1) , padding =\"same\" , activation='relu',kernel_initializer='he_uniform')(x2)\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(16,(3,3), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x1)\n",
        "x2 = Conv2DTranspose(16,(3,3), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x2)\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(32,(3,3), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x1)\n",
        "x2 = Conv2DTranspose(32,(3,3), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x2)\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(64,(2,2), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x1)\n",
        "x2 = Conv2DTranspose(64,(2,2), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x2)\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(1,(3,3), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x1)\n",
        "x2 = Conv2DTranspose(1,(3,3), strides=(1,1) , activation='relu',kernel_initializer='he_uniform')(x2)\n",
        "\n",
        "x1 = UpSampling2D(size=(8,8), interpolation='bilinear')(x1)\n",
        "x2 = UpSampling2D(size=(8,8), interpolation='bilinear')(x2)\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "x = Dropout(0.5)(x)\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='softmax')(x)\n",
        "prediction = Reshape((112,112))(x)\n",
        "\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "  \n",
        "VL2_model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.01),metrics=['accuracy'])\n",
        "#VL2_model.compile(loss=focal_loss,optimizer=optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
        "VL2_model.fit(x=[input_train01,input_train02],y=output_train,steps_per_epoch=20,epochs=3,validation_data=[[input_test01,input_test02],output_test],validation_steps=5)\n",
        "\n",
        "target_dir = '/content/drive/My Drive/modelo/'\n",
        "if not os.path.exists(target_dir):\n",
        "    os.mkdir(target_dir)\n",
        "VL2_model.save('/content/drive/My Drive/modelo/modelo09.h5')\n",
        "VL2_model.save_weights('/content/drive/My Drive/modelo/pesos09.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:114: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"re...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "20/20 [==============================] - 629s 31s/step - loss: 0.4456 - acc: 0.9624 - val_loss: 0.4415 - val_acc: 0.9622\n",
            "Epoch 2/3\n",
            "20/20 [==============================] - 619s 31s/step - loss: 0.4456 - acc: 0.9624 - val_loss: 0.4415 - val_acc: 0.9622\n",
            "Epoch 3/3\n",
            "20/20 [==============================] - 619s 31s/step - loss: 0.4456 - acc: 0.9624 - val_loss: 0.4415 - val_acc: 0.9622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdfvYEA1o-GE",
        "colab_type": "code",
        "outputId": "4dce3327-1c7c-4548-ae84-71e95bc49a1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg19 import VGG19\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import concatenate ,Convolution2D ,Conv2DTranspose ,Reshape ,Dropout ,BatchNormalization ,UpSampling2D\n",
        "from keras import backend as K\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/Data_VL2/dataVL2.txt', header = None)\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "input01 = data[0]\n",
        "input02 = data[1]\n",
        "output = data[2]\n",
        "\n",
        "def data_gen(path,i,inicio,fin):\n",
        "  datagen = []\n",
        "  if i==0:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir)/255  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  if i==1:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir,cv2.IMREAD_GRAYSCALE)/255  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  return datagen\n",
        "\n",
        "input_train01 = np.array(data_gen(input01,0,0,400))\n",
        "input_train02 = np.array(data_gen(input02,0,0,400))\n",
        "output_train = np.array(data_gen(output,1,0,400))\n",
        "\n",
        "input_test01 = np.array(data_gen(input01,0,400,500))\n",
        "input_test02 = np.array(data_gen(input02,0,400,500))\n",
        "output_test = np.array(data_gen(output,1,400,500))\n",
        "\n",
        "#RED NEURONAL\n",
        "\n",
        "#sess = tf.InteractiveSession()\n",
        "\n",
        "model1 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "model2 = VGG19(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(16,(1,1), strides=(16,16) , padding =\"same\" , activation='relu')(x1)\n",
        "x2 = Conv2DTranspose(16,(1,1), strides=(16,16) , padding =\"same\" , activation='relu')(x2)\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='softmax')(x)\n",
        "prediction = Reshape((112,112))(x)\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "  \n",
        "VL2_model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.01),metrics=['accuracy'])\n",
        "#VL2_model.compile(loss=focal_loss,optimizer=optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
        "VL2_model.fit(x=[input_train01,input_train02],y=output_train,steps_per_epoch=20,epochs=3,validation_data=[[input_test01,input_test02],output_test],validation_steps=5)\n",
        "\n",
        "target_dir = '/content/drive/My Drive/modelo/'\n",
        "if not os.path.exists(target_dir):\n",
        "    os.mkdir(target_dir)\n",
        "VL2_model.save('/content/drive/My Drive/modelo/modelo10.h5')\n",
        "VL2_model.save_weights('/content/drive/My Drive/modelo/pesos10.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"re...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/3\n",
            "20/20 [==============================] - 629s 31s/step - loss: 0.4386 - acc: 0.9626 - val_loss: 0.4697 - val_acc: 0.9613\n",
            "Epoch 2/3\n",
            "20/20 [==============================] - 630s 31s/step - loss: 0.4386 - acc: 0.9626 - val_loss: 0.4697 - val_acc: 0.9613\n",
            "Epoch 3/3\n",
            "20/20 [==============================] - 627s 31s/step - loss: 0.4386 - acc: 0.9626 - val_loss: 0.4697 - val_acc: 0.9613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4rSpnIQWmUr",
        "colab_type": "code",
        "outputId": "df526982-cfd9-4c1e-8db1-a27e6e60a791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg16 import VGG16\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import concatenate ,Convolution2D ,Conv2DTranspose ,Reshape ,Dropout ,BatchNormalization ,UpSampling2D\n",
        "from keras import backend as K\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/Data_VL2/dataVL2.txt', header = None)\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "input01 = data[0]\n",
        "input02 = data[1]\n",
        "output = data[2]\n",
        "\n",
        "def data_gen(path,i,inicio,fin):\n",
        "  datagen = []\n",
        "  if i==0:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir)/255  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  if i==1:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir,cv2.IMREAD_GRAYSCALE)/255  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  return datagen\n",
        "\n",
        "input_train01 = np.array(data_gen(input01,0,0,450))\n",
        "input_train02 = np.array(data_gen(input02,0,0,450))\n",
        "output_train = np.array(data_gen(output,1,0,450))\n",
        "\n",
        "input_test01 = np.array(data_gen(input01,0,450,500))\n",
        "input_test02 = np.array(data_gen(input02,0,450,500))\n",
        "output_test = np.array(data_gen(output,1,450,500))\n",
        "\n",
        "#RED NEURONAL\n",
        "\n",
        "#sess = tf.InteractiveSession()\n",
        "\n",
        "model1 = VGG16(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "model2 = VGG16(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(16,(1,1), strides=(16,16) , padding =\"same\" , activation='relu')(x1)\n",
        "x2 = Conv2DTranspose(16,(1,1), strides=(16,16) , padding =\"same\" , activation='relu')(x2)\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='softmax')(x)\n",
        "prediction = Reshape((112,112))(x)\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "  \n",
        "VL2_model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.1),metrics=['accuracy'])\n",
        "#VL2_model.compile(loss=focal_loss,optimizer=optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
        "VL2_model.fit(x=[input_train01,input_train02],y=output_train,steps_per_epoch=20,epochs=3,validation_data=[[input_test01,input_test02],output_test],validation_steps=5)\n",
        "\n",
        "target_dir = '/content/drive/My Drive/modelo/'\n",
        "if not os.path.exists(target_dir):\n",
        "    os.mkdir(target_dir)\n",
        "VL2_model.save('/content/drive/My Drive/modelo/modelo12.h5')\n",
        "VL2_model.save_weights('/content/drive/My Drive/modelo/pesos12.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"re...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 450 samples, validate on 50 samples\n",
            "Epoch 1/3\n",
            "20/20 [==============================] - 621s 31s/step - loss: 0.4460 - acc: 0.9622 - val_loss: 0.4339 - val_acc: 0.9632\n",
            "Epoch 2/3\n",
            "20/20 [==============================] - 584s 29s/step - loss: 0.4460 - acc: 0.9622 - val_loss: 0.4339 - val_acc: 0.9632\n",
            "Epoch 3/3\n",
            "20/20 [==============================] - 581s 29s/step - loss: 0.4460 - acc: 0.9622 - val_loss: 0.4339 - val_acc: 0.9632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr4duhvmW7T2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "#import keras \n",
        "from keras import optimizers\n",
        "from keras.applications.vgg16 import VGG16\n",
        "#from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from keras.models import Model \n",
        "from keras.layers import concatenate ,Convolution2D ,Conv2DTranspose ,Reshape ,Dropout ,BatchNormalization ,UpSampling2D\n",
        "from keras import backend as K\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/Data_VL2/dataVL2.txt', header = None)\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "input01 = data[0]\n",
        "input02 = data[1]\n",
        "output = data[2]\n",
        "\n",
        "def data_gen(path,i,inicio,fin):\n",
        "  datagen = []\n",
        "  if i==0:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir)/255  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  if i==1:\n",
        "    for j in range(inicio,fin):\n",
        "      img_dir = '/content/drive/My Drive/Data_VL2'+path[j]\n",
        "      img_array = cv2.imread(img_dir,cv2.IMREAD_GRAYSCALE)/255  # convert to array #,cv2.IMREAD_GRAYSCALE\n",
        "      datagen.append(img_array)  # add this to our training_data\n",
        "  return datagen\n",
        "\n",
        "input_train01 = np.array(data_gen(input01,0,0,450))\n",
        "input_train02 = np.array(data_gen(input02,0,0,450))\n",
        "output_train = np.array(data_gen(output,1,0,450))\n",
        "\n",
        "input_test01 = np.array(data_gen(input01,0,450,500))\n",
        "input_test02 = np.array(data_gen(input02,0,450,500))\n",
        "output_test = np.array(data_gen(output,1,450,500))\n",
        "\n",
        "#RED NEURONAL\n",
        "\n",
        "#sess = tf.InteractiveSession()\n",
        "\n",
        "model1 = VGG16(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "model2 = VGG16(weights = \"imagenet\", include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
        "for layer in model1.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "   layer.trainable = False\n",
        "\n",
        "for layer in model2.layers:\n",
        "    layer.name = layer.name + str(\"_2\")\n",
        "\n",
        "#Adding custom Layers \n",
        "x1 = model1.output\n",
        "x2 = model2.output\n",
        "\n",
        "x1 = BatchNormalization()(x1)\n",
        "x2 = BatchNormalization()(x2)\n",
        "\n",
        "x1 = Conv2DTranspose(16,(1,1), strides=(16,16) , padding =\"same\" , activation='relu')(x1)\n",
        "x2 = Conv2DTranspose(16,(1,1), strides=(16,16) , padding =\"same\" , activation='relu')(x2)\n",
        "\n",
        "x1 = Dropout(0.7)(x1)\n",
        "x2 = Dropout(0.7)(x2)\n",
        "\n",
        "# combine the output of the two branches\n",
        "x = concatenate([x1, x2])\n",
        "x = Convolution2D(1,(1,1), padding =\"same\", activation='softmax')(x)\n",
        "prediction = Reshape((112,112))(x)\n",
        "# creating the final model \n",
        "VL2_model = Model(input = [model1.input , model2.input], output = prediction)\n",
        "  \n",
        "VL2_model.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=0.1),metrics=['accuracy'])\n",
        "#VL2_model.compile(loss=focal_loss,optimizer=optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
        "VL2_model.fit(x=[input_train01,input_train02],y=output_train,steps_per_epoch=20,epochs=3,validation_data=[[input_test01,input_test02],output_test],validation_steps=5)\n",
        "\n",
        "target_dir = '/content/drive/My Drive/modelo/'\n",
        "if not os.path.exists(target_dir):\n",
        "    os.mkdir(target_dir)\n",
        "VL2_model.save('/content/drive/My Drive/modelo/modelo13.h5')\n",
        "VL2_model.save_weights('/content/drive/My Drive/modelo/pesos13.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdMssqc9l_Ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}